
The models were built using three different approaches: 1. Tree-based predictive models i. Decision trees (Cubist) ii. Random forests (randomForest) 2. Artificial neural networks (ANNs) (monmlp) 3. Symbolic regression (rgp).
Cubist is a tree-based modeling approach wherein a linear model is fit iteratively, resulting in a set of linear models at each node starting from the root to the last node of the tree. Cubist is an ensemble-based technique. All variables that are covered by a linear function at a single node are then discarded from the future iterations for that particular tree. This process is recursively continued until all the input variables have been covered by a single or a set of rules in a tree. This is also known as the separate-and-conquer approach. At every step, the response of the model is analyzed and adjusted to be used by the next model, until the final model is achieved. The final prediction is a simple average of the predictions from each tree in the ensemble. 25 random forest
Random forests are well known tree-based models. One model is shaped by numerous trees in the collection. Independent trees are generated based on sample data randomly taken out of the training data set. Errors exhibited by individual trees are collectively representative of the generalization error of the randomForest model. 26 An extensive search for the best architecture was carried out using a 10cv scheme, followed by training of the best-performing architecture on the full data set.
Monmlp is the implementation of ANN in R. ANNs withmonmlp are generalized feed-forward ANNs that work in a monotone fashion using the backpropagation (BP) algorithm
Khalid et al enhanced by the nonlinear minimization algorithm (nlm) for training. Neural networks imitate the structure of the human brain, whereby information is passed on between neurons in the form of synapses. Monmlp allows two hidden layers of neurons, with hyperbolic tangent and linear activation as transfer functions. 27 In monmlp, signals move forward from the input layer through sigmoid neurons before reaching the linear output layer.
Moreover, different activation functions, such as hyperbolic tangent sigmoid (TAN-SIG) transfer function, logarithm sigmoid (LOGSIG) transfer function, and pure linear (PURELIN) transfer function, are the potential transfer functions that can be used in each layer. 28 The neural network package monmlp was run extensively to find the bestperforming network architecture with 10cv. The whole data set was used to retrain the model on the best architecture.
Genetic programming (GP) is a bioinspired algorithm based on evolution principles to solve complex problems. A complex problem is broken down into smaller, simpler problems with random solutions. These solutions are then evolved through the biologically mimicked process of variation and selection until the end condition is reached or a workable solution is generated. Thus, rgp is an implementation of GP methods in the R environment. 29 Package rgp results are simple representations of the problem without being exposed to a priori information. rgp offers various options for initialization, variation, and selection procedures inherent in GP. GP implements a tree structure representation to show the mathematical equations. The tree structure is composed of two parts: function set (nodes) and a terminal set (leaves). 30 The function set can be chosen through the operators {+, -, *, /, sin, cos, log, abs}, mathematical functions, conditional statements, or even the user-defined operators. In addition, the terminal set includes constants and model variables. 31,32 To find the best solution, different runs were conducted by iterating over GP tuning parameters. The most important tuning parameters are maximum size of the chromosome, the number of generations, and population size. 30 The size of the chromosome, which governs the maximum length of the equation, was varied from 5 to 100. The population size was set to 1,000, and the number of generations was set to 500 million evolution steps divided into 100 stages.
These equations can be optimized by different strategies. 33,34 For this experiment, equations were created on the whole data set, and then selected ones were optimized using the simulated annealing (SANN) algorithm, followed by a quasiNewton (Broyden-Fletcher-Goldfarb-Shanno [BFGS]) method. 35,36 independent feature selection by fscaret
The package fscaret allows semiautomatic feature selection, working as a wrapper for the caret package in R. Fscaret is specialized for in silico feature selection experiments, whereby approximately 120 different packages are used to fit models. 37 Input feature ranking is extracted from trained models by using weighted averages. Sum of squared errors (SSE), mean squared errors (MSEs), and RMSEs are used to evaluate models. fscaret has been successfully implemented in recent studies. 38,39
Data from additional tableting experiments was used to test the trained models. Tablets were made with MCC. The external data set was particularly challenging for the models because roll compaction conditions were kept constant in the training data set while being systematically changed in the external data set. Table 1 shows the results of external validation. The models were able to predict porosity with substantial accuracy using unseen input data. The external data set is composed of experiments with two different roll compactor tooling (cheek plates and rim rolls) and two different roll compaction forces (4 kN and 8 kN) for each. The variety of roll compactor settings introduces variance in the external data set unknown to the trained models. Although CI systems are not known for their extrapolation abilities, 42 the models presented herein are able to extrapolate successfully in a small range. Regardless of the complexity in the external data set, the rgp model represented by Equation 4 makes accurate predictions of the test cases from all different tooling and force conditions (Figure 4). Table 4 shows the levels of accuracy of prediction for the external validation data set. Non-roll-compacted powder is predicted with the most accuracy (NRMSE: 7%), followed by prediction of roll compaction using rim rolls at 4 kN (NRMSE: 9%).
Further RSM analysis based on the GP equation shows that variation in the amount of MCC does not have a profound effect on the porosity of the tablet, while an increase in the die compaction force considerably reduces the porosity of tablets Figure 6), a behavior that can be attributed to particle rearrangement followed by plastic deformation during the compression stage. 44 Feature selection on the external data set to establish the surplus of discarded variables Plausible performance of the trained CI models on a significantly different external data set could be an occurrence of chance. To study this in detail, important features from both the data sets were compared. Feature selection experiments using fscaret were run independently on the external data set. Confirmatory models were developed with a variation of input vectors using the tree-based learning method Cubist. The resulting top features of the external data set are in agreement with the features used to train the models. The results confirm that the die compaction force is the most important variable, with roll compaction force, roll compaction tooling,percentage of material in the formulation, and granule fraction range displaying much lower ranking. Models developed without the information of roll compaction force, roll compaction tooling, and granule fraction range predict porosity of tablets accurately (Table 5: experiments with two and three inputs). The successful modeling shows that these variables do not contribute to the outcome within this design space. The results strongly suggest redundancies in the original data set, which, once removed, do not affect the prediction of porosity within the factor space. Correlation analysis of the original data set reveals that porosity is highly correlated to die compaction force (Figure 7).a nrMse and R 2 are presented. all experiments were done in 10cv mode of data representation. Mlr and decision trees were implemented in r using lm command and cubist package, respectively. Abbreviations: 10cv, tenfold cross-validation; Mlr, multiple linear regression; nrMse, normalized root-mean-square error.
